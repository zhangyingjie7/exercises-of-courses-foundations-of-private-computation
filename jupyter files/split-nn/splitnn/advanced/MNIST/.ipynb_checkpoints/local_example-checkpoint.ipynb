{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "interracial-myanmar",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import syft as sy\n",
    "import torch\n",
    "\n",
    "from six.moves import urllib\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "urllib.request.install_opener(opener)\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "trainset = datasets.MNIST('mnist', download=True, train=True, transform=transform)\n",
    "# valset = datasets.MNIST('mnist', download=True, train=False, transform=transform)\n",
    "\n",
    "\n",
    "#Take first n images\n",
    "data = trainset.data.view(trainset.data.shape[0], -1).float()\n",
    "data = data\n",
    "dataloader = torch.utils.data.DataLoader(data, batch_size=1024)\n",
    "\n",
    "#Take first n targets\n",
    "targets = trainset.targets\n",
    "targetloader = torch.utils.data.DataLoader(targets, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "configured-anthropology",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Images are aligned\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(trainset.data[1].numpy().squeeze(), cmap='gray_r')\n",
    "print(targets[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "returning-shield",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deploy same model as original tutorial\n",
    "\n",
    "input_size = 784\n",
    "hidden_sizes = [128, 640]\n",
    "output_size = 10\n",
    "\n",
    "class SyNet_client(sy.Module):\n",
    "    def __init__(self, torch_ref):\n",
    "        super(SyNet_client, self).__init__(torch_ref=torch_ref)\n",
    "        self.lin1 = nn.Linear(input_size, hidden_sizes[0])\n",
    "        self.lin2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = self.torch_ref.nn.functional.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.torch_ref.nn.functional.relu(x)\n",
    "        return x\n",
    "    \n",
    "class SyNet_server(sy.Module):\n",
    "    def __init__(self, torch_ref):\n",
    "        super(SyNet_server, self).__init__(torch_ref=torch_ref)\n",
    "        self.lin3 = nn.Linear(hidden_sizes[1], output_size)\n",
    "        self.sft = nn.LogSoftmax(dim=1) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.lin3(x)\n",
    "        x = self.sft(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "manufactured-junction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 1\n",
    "model1 = SyNet_client(torch)\n",
    "# model1_ptr = model1.send(duet)\n",
    "opt1 = torch.optim.SGD(params=model1.parameters(),lr=0.3)\n",
    "\n",
    "#Model 2\n",
    "model2 = SyNet_server(torch)\n",
    "opt2 = torch.optim.SGD(params=model2.parameters(),lr=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "buried-custom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(13.2089, grad_fn=<NllLossBackward>)\n",
      "tensor(474930.3438, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0354e+08, grad_fn=<NllLossBackward>)\n",
      "tensor(12150485., grad_fn=<NllLossBackward>)\n",
      "tensor(1832.1757, grad_fn=<NllLossBackward>)\n",
      "tensor(5.6498, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8869, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6026, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3981, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3399, grad_fn=<NllLossBackward>)\n",
      "tensor(3.1032, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3155, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3156, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3117, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3097, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3101, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3111, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3038, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3091, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3086, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3054, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3073, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3017, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3055, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3020, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3036, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3044, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3038, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3047, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3065, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3037, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3025, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3022, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3056, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3031, grad_fn=<NllLossBackward>)\n",
      "tensor(310.2244, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3024, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3010, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3024, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3037, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3017, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3022, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3044, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3049, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3038, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3027, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2995, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3030, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3043, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3039, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3034, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3004, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3042, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3032, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3044, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3030, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3013, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3026, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3019, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "for image, target in zip(dataloader, targetloader):    \n",
    "    opt1.zero_grad()\n",
    "    opt2.zero_grad()\n",
    "    \n",
    "    activation = model1(image)\n",
    "    pred = model2(activation)\n",
    "    \n",
    "    loss = torch.nn.functional.nll_loss(pred, target)\n",
    "    loss.backward()\n",
    "    \n",
    "    opt1.step()\n",
    "    opt2.step()\n",
    "    \n",
    "    print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-exercise",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-replacement",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
